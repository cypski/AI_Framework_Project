{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_results = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_results = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_results['Framework'] = word_counts_results['Filename'].str.split('_').str[0]\n",
    "word_counts_results['Date'] = word_counts_results['Filename'].str.split('_').str[1]\n",
    "new_order = ['Filename', 'Framework', 'Date', \"risk\", \"safe\", \"bias\", \"security\", \"ethic\",\n",
    "                      \"accountab\", \"transparen\", \"explainab\", \"policy\",\n",
    "                      \"compliance\", \"governance\", \"protect\", \"sustainab\",\n",
    "                      \"fair\", \"catastroph\", \"responsib\", \"prepare\"]\n",
    "word_counts_results = word_counts_results[new_order]\n",
    "\n",
    "word_counts_results = word_counts_results.merge(length_results, on = 'Filename')\n",
    "word_counts_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_0_frameworks = ['Anthropic', 'Cohere', 'Deepmind', 'OpenAI', 'Naver', 'META', 'xAI', 'Microsoft']\n",
    "group_1_frameworks = ['Deloitte', 'G42', 'Grammarly', 'IBM', 'KPMG', 'Magic', 'NVDIA', 'PaloAlto', 'PwC', 'Amazon']\n",
    "\n",
    "group_0_results = word_counts_results[word_counts_results['Framework'].isin(group_0_frameworks)]\n",
    "group_1_results = word_counts_results[word_counts_results['Framework'].isin(group_1_frameworks)]\n",
    "\n",
    "word_totals_group_0 = group_0_results.iloc[:, 3:].sum(axis = 0).reset_index()\n",
    "word_totals_group_0.columns = ['Word', 'Total']\n",
    "word_totals_group_0 = word_totals_group_0[word_totals_group_0['Word'] != 'Length']  \n",
    "word_totals_group_0_sorted = word_totals_group_0.sort_values(by = 'Total', ascending = False)\n",
    "\n",
    "word_totals_group_1 = group_1_results.iloc[:, 3:].sum(axis = 0).reset_index()\n",
    "word_totals_group_1.columns = ['Word', 'Total']\n",
    "word_totals_group_1 = word_totals_group_1[word_totals_group_1['Word'] != 'Length']\n",
    "word_totals_group_1_sorted = word_totals_group_1.sort_values(by = 'Total', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = {\n",
    "    'risk': 'Risk', \n",
    "    'security': 'Security',\n",
    "    'safe': 'Safe/Safety',\n",
    "    'responsib': 'Responsible/Responsibility',\n",
    "    'ethic': 'Ethics/Ethical',\n",
    "    'transparen': 'Tranaparent/Transparency',\n",
    "    'governance': 'Governance',\n",
    "    'policy': 'Policy/Policymaker',\n",
    "    'bias': 'Bias',\n",
    "    'protect': 'Protect/Protection',\n",
    "    'compliance': 'Compliance',\n",
    "    'catastroph': 'Catastrophe/Catastrophic',\n",
    "    'fair': 'Fair/Fairness',\n",
    "    'prepare': 'Prepare/Preparedness',\n",
    "    'accountab': 'Accountable/Accountability',\n",
    "    'explainab': 'Explainable/Explainability',\n",
    "    'sustainab': 'Sustainable/Sustainability',\n",
    "}\n",
    "\n",
    "\n",
    "word_totals_group_0_sorted['Word'] = word_totals_group_0_sorted['Word'].replace(word_mapping)\n",
    "word_totals_group_1_sorted['Word'] = word_totals_group_1_sorted['Word'].replace(word_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6), dpi = 500)\n",
    "plt.bar(word_totals_group_0_sorted['Word'], word_totals_group_0_sorted['Total'], color = 'skyblue')\n",
    "plt.xticks(rotation = 45, ha = 'right')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Total Count')\n",
    "plt.title('Total Word Counts Across All Frameworks')\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi = 500)\n",
    "plt.bar(word_totals_group_1_sorted['Word'], word_totals_group_1_sorted['Total'], color = 'skyblue')\n",
    "plt.xticks(rotation = 45, ha = 'right')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Total Count')\n",
    "plt.title('Total Word Counts Across All Frameworks')\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = word_counts_results.melt(\n",
    "    id_vars=[\"Filename\", \"Framework\", \"Date\", 'Length'], \n",
    "    var_name=\"Word\", \n",
    "    value_name=\"Count\"\n",
    ")\n",
    "\n",
    "melted_df['Word'] = melted_df['Word'].replace(word_mapping)\n",
    "\n",
    "melted_df['Normalised_Count'] = melted_df['Count'] / melted_df['Length']\n",
    "\n",
    "melted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_mapping = {\n",
    "    'Amazon': 0, 'Anthropic': 1, 'Cohere': 1,\n",
    "    'Deepmind': 1, 'Deloitte': 0, 'G42': 0,\n",
    "    'Grammarly': 0, 'IBM': 0, 'KPMG': 0, 'Magic': 0,\n",
    "    'META': 1, 'Microsoft': 1,'Naver': 1, 'NVDIA': 0,\n",
    "    'OpenAI': 1, 'PaloAlto': 0, 'PwC': 0, 'xAI': 1\n",
    "}\n",
    "\n",
    "melted_df['Frontier'] = melted_df['Framework'].map(frontier_mapping)\n",
    "\n",
    "grouped_words = melted_df.groupby(['Frontier', 'Word'])['Normalised_Count'].mean().reset_index()\n",
    "grouped_words['Frontier'] = grouped_words['Frontier'].astype(int)\n",
    "\n",
    "top_words = (\n",
    "    grouped_words.groupby('Frontier', group_keys = False)\n",
    "    .apply(lambda x: x.nlargest(8, 'Normalised_Count'))\n",
    ")\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = grouped_words.pivot(index = 'Word', columns = 'Frontier', values = 'Normalised_Count').fillna(0)\n",
    "\n",
    "plt.figure(figsize = (10, 8), dpi = 500)\n",
    "sns.heatmap(heatmap_data, annot = True, \n",
    "            fmt = \".4f\", cmap = \"Reds\", \n",
    "            cbar_kws = {'label': 'Normalized Count (%)'}, \n",
    "            vmin = 0, vmax = heatmap_data.values.max() * 1.5\n",
    ")\n",
    "plt.title('Word Usage Heatmap: Frontier vs Non-Frontier Companies')\n",
    "plt.ylabel('Word')\n",
    "plt.xlabel('Frontier')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = (\n",
    "    melted_df.groupby(\"Framework\", group_keys=False)\n",
    "    .apply(lambda x: x.nlargest(5, \"Count\"))\n",
    ")\n",
    "\n",
    "top_words.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fwenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
